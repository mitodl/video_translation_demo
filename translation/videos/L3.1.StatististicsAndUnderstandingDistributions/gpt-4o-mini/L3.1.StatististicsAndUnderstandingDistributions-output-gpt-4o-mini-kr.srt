1
00:00:05,053 --> 00:00:06,220
강사: 여러분, 안녕하세요.

2
00:00:06,220 --> 00:00:07,520
다시 오신 것을 환영합니다.

3
00:00:07,520 --> 00:00:09,760
오늘 강의에서는 우리가 통계에 대해 이야기할 것입니다.

4
00:00:09,760 --> 00:00:14,120
데이터 탐색과 분석을 위한 통계에 대해서요.

5
00:00:14,120 --> 00:00:16,880
간단한 질문으로 시작해 보겠습니다.

6
00:00:16,880 --> 00:00:18,920
통계란 무엇인가요?

7
00:00:18,920 --> 00:00:22,680
정식으로, 통계는 데이터를 수집하고,

8
00:00:22,680 --> 00:00:25,540
분석하고, 해석하고,

9
00:00:25,540 --> 00:00:29,920
제시하여 인사이트를 도출하고

10
00:00:29,920 --> 00:00:31,720
더 나은 결정을 지원하는 수학의 한 분야입니다.

11
00:00:31,720 --> 00:00:36,360
이번 강의에서는 기술 통계, 즉

12
00:00:36,360 --> 00:00:39,120
데이터를 요약하고 시각화하여

13
00:00:39,120 --> 00:00:42,480
패턴과 경향을 드러내는 방법에 초점을 맞추겠습니다.

14
00:00:42,480 --> 00:00:45,040
예시로 시작해 보겠습니다.

15
00:00:45,040 --> 00:00:48,400
학생들의 신장을 연구하고 싶다고 가정해 봅시다.

16
00:00:48,400 --> 00:00:51,560
유니버설 AI에 등록된 학생들 말입니다.

17
00:00:51,560 --> 00:00:56,720
UAI의 모든 학생의 신장을 측정하는 대신,

18
00:00:56,720 --> 00:00:58,400
우리는 샘플을 취합니다--

19
00:00:58,400 --> 00:01:02,650
예를 들어, 무작위로 선정된 100명의 학생을요.

20
00:01:02,650 --> 00:01:07,730
이로써 우리는 100개의 신장 데이터 세트를 얻습니다.

21
00:01:07,730 --> 00:01:11,090
이제 이 데이터를 시각화하기 시작할 수 있습니다,

22
00:01:11,090 --> 00:01:15,370
예를 들어, 샘플된 모든 학생의 신장을

23
00:01:15,370 --> 00:01:17,690
나란히 도표로 나타내는 방식으로요.

24
00:01:17,690 --> 00:01:20,770
물론, 값들을 정렬하여

25
00:01:20,770 --> 00:01:26,050
경향을 파악할 수 있습니다. 즉, 대부분의 학생들이 비슷한 신장을 갖고 있다는 것이죠.

26
00:01:26,050 --> 00:01:29,810
그렇다면 일반적인 학생의 신장은 얼마일까요?

27
00:01:29,810 --> 00:01:33,650
세 가지 일반적인 방법으로 설명할 수 있습니다.

28
00:01:33,650 --> 00:01:38,350
산술평균 신장을 계산할 수 있습니다. 평균은

29
00:01:38,350 --> 00:01:41,290
65인치입니다.

30
00:01:41,290 --> 00:01:43,770
다음으로, 중앙값을 찾을 수 있습니다. 중앙값은

31
00:01:43,770 --> 00:01:47,890
데이터가 정렬될 때 중간 값을 나타냅니다.

32
00:01:47,890 --> 00:01:52,530
즉, 50명의 학생이 이 값보다 작고 50명의 학생이

33
00:01:52,530 --> 00:01:55,530
이 값보다 큽니다.

34
00:01:55,530 --> 00:01:58,890
마지막으로, 최빈값도 식별할 수 있습니다. 이것은

35
00:01:58,890 --> 00:02:02,410
가장 빈번한 값입니다.

36
00:02:02,410 --> 00:02:07,070
이 신장들을 동일한 크기의 구간으로 묶고

37
00:02:07,070 --> 00:02:10,389
각 구간에 들어가는 학생 수를 셉니다.

38
00:02:10,389 --> 00:02:12,750
이러한 카운팅과 구간화 과정이

39
00:02:12,750 --> 00:02:15,870
분포를 탐색하는 시각화 기술의 기초입니다.

40
00:02:15,870 --> 00:02:18,990
여기에서 우리는 대부분의 학생 신장이 어디에 집중되어 있는지

41
00:02:18,990 --> 00:02:22,790
그리고 범위에 걸쳐 어떻게 퍼져 있는지 볼 수 있습니다.

42
00:02:22,790 --> 00:02:26,670
몇몇 개인은 상대적으로 키가 크지만,

43
00:02:26,670 --> 00:02:29,210
대부분의 신장은 중앙 주위에 모여 있습니다.

44
00:02:29,210 --> 00:02:33,390
이제 개별 점 대신,

45
00:02:33,390 --> 00:02:36,370
막대 차트를 사용하여 히스토그램을 만들 수 있습니다.

46
00:02:36,370 --> 00:02:41,670
히스토그램은 각 구간에서 값이 얼마나 자주 발생하는지를 보여주고,

47
00:02:41,670 --> 00:02:46,390
우리의 경우 신장 범위입니다.

48
00:02:46,390 --> 00:02:49,910
이것은 샘플의 분포에 대한 명확한 그림을 제공합니다.

49
00:02:49,910 --> 00:02:54,910
대부분의 학생들이 어디에 있는지, 나머지 학생들이 어떻게 퍼져 있는지 알려줍니다.

50
00:02:54,910 --> 00:03:00,190
우리는 구간 크기를 변경할 수 있습니다--

51
00:03:00,190 --> 00:03:03,230
적분 크기를-- 데이터에서 얼마나 많은

52
00:03:03,230 --> 00:03:06,040
변동성이 보이는지 확인하기 위해서요.

53
00:03:06,040 --> 00:03:09,120
작은 구간 크기는 더 높은 세분화에서 변동성을 보여주며,

54
00:03:09,120 --> 00:03:11,600
큰 구간 크기는

55
00:03:11,600 --> 00:03:15,880
보이는 세부 정보를 줄입니다.

56
00:03:15,880 --> 00:03:18,200
그래서 만약 구간이 너무 작으면 결과는

57
00:03:18,200 --> 00:03:22,120
잡음이 될 수 있으며, 너무 크면,

58
00:03:22,120 --> 00:03:26,020
분포가 지나치게 단순화될 수 있습니다.

59
00:03:26,020 --> 00:03:30,080
우리는 커널 밀도 추정(Kernel Density Estimation), 즉 KDE를 사용하여

60
00:03:30,080 --> 00:03:34,920
데이터를 제시할 수도 있습니다.

61
00:03:34,920 --> 00:03:39,080
KDE는 전통적인 히스토그램에 비해 부드러운 대안을 제공합니다.

62
00:03:39,080 --> 00:03:41,760
히스토그램과 달리 KDE는 구간 크기에 의존하지 않고

63
00:03:41,760 --> 00:03:46,920
데이터 분포의 형태를 더 명확하게 시각화합니다.

64
00:03:46,920 --> 00:03:50,720
또 다른 대안은 바이올린 플롯으로,

65
00:03:50,720 --> 00:03:53,240
거울에 비친 KDE입니다.

66
00:03:53,240 --> 00:03:59,440
두 분포를 비교하는 데 유용합니다.

67
00:03:59,440 --> 00:04:03,120
우리의 예에서 남학생과 여학생의 신장 분포와 같은 경우요.

68
00:04:03,120 --> 00:04:06,260
데이터 형태에 대해 이야기하면서, 왜도(skewness)는 유용한 측정 지표입니다.

69
00:04:06,260 --> 00:04:09,220
왜도는 데이터 분포의 비대칭성, 또는 기울기를 포착합니다.

70
00:04:09,220 --> 00:04:13,780
이는 데이터가 편향되었는지, 또는 분포의 한쪽에 위치한지 이해하는 데 도움을 줍니다.

71
00:04:13,780 --> 00:04:19,899
예를 들어, 포인트가 중심 주위에 고르게 분포되어 있다면,

72
00:04:19,899 --> 00:04:23,940
데이터는 대칭적이며 왜도가 없습니다.

73
00:04:23,940 --> 00:04:25,980
대부분의 데이터 포인트가 왼쪽에 모여 있고,

74
00:04:25,980 --> 00:04:29,020
오른쪽에 긴 꼬리가 있으면 이를 양의 왜도(positive skew)라고 부릅니다.

75
00:04:29,020 --> 00:04:31,380
대부분의 포인트가 오른쪽에 모여 있고,

76
00:04:31,380 --> 00:04:34,300
왼쪽에 긴 꼬리가 있으면 부의 왜도(negative skew)입니다.

77
00:04:34,300 --> 00:04:37,740
완벽하게 대칭적인 분포에서는,

78
00:04:37,740 --> 00:04:43,020
평균, 중앙값, 최빈값이 동일합니다.

79
00:04:43,020 --> 00:04:45,980
하지만 비대칭적이거나 왜도가 있는 분포에서는

80
00:04:45,980 --> 00:04:50,260
이 세 가지 측정값이 다릅니다.

81
00:04:50,260 --> 00:04:52,980
이제 이상치에 대해 이야기해 보겠습니다.

82
00:04:52,980 --> 00:04:57,300
이상치는 데이터 세트에서 비정상적으로 높거나 낮은 값입니다.

83
00:04:57,300 --> 00:05:01,300
많은 경우, 이상치는 예상되는 것이지만,

84
00:05:01,300 --> 00:05:04,500
오류로 인해 발생할 수도 있습니다.

85
00:05:04,500 --> 00:05:07,590
예를 들어, 학생들에게 신장을 보고하도록 요청하여

86
00:05:07,590 --> 00:05:13,710
조사를 하고 있다고 가정해 봅시다. 그러던 중 한 학생이

87
00:05:13,710 --> 00:05:17,050
농담으로 자신의 신장을 에펠탑의 높이라고 보고합니다.

88
00:05:17,050 --> 00:05:19,670
이제 결과를 도표로 그리면, 히스토그램은

89
00:05:19,670 --> 00:05:22,830
이런 식으로 보일 수 있습니다.

90
00:05:22,830 --> 00:05:26,190
우리의 평균은 이제 왜곡되었습니다.

91
00:05:26,190 --> 00:05:29,390
190인치가 넘게 증가했습니다.

92
00:05:29,390 --> 00:05:34,630
이는 단일 이상치가 평균에 얼마나 큰 영향을 미칠 수 있는지를 보여줍니다.

93
00:05:34,630 --> 00:05:37,230
그러나 중앙값은 약 65인치로 남아 있어

94
00:05:37,230 --> 00:05:39,590
데이터의 진정한 중심 경향을 반영합니다.

95
00:06:00,630 --> 00:06:05,030
데이터 세트를 살펴보면, 이 매우 높은

96
00:06:05,030 --> 00:06:08,290
값이 명백하게 오류임을 알 수 있습니다.

97
00:06:08,290 --> 00:06:12,730
데이터를 효과적으로 분석하기 위해서는 안전하게 제거할 수 있습니다.

98
00:06:12,730 --> 00:06:16,330
이것이 분석을 수행하거나 결론을 도출하기 전에

99
00:06:16,330 --> 00:06:21,250
데이터를 시각적으로 검사하는 것이 항상 중요한 이유입니다.

100
00:06:21,250 --> 00:06:24,290
모든 것이 의미가 있는지 확인하십시오.

101
00:06:24,290 --> 00:06:27,850
그럼 이제 우리 기술 통계로 돌아가서,

102
00:06:27,850 --> 00:06:32,370
데이터가 대칭적일 때 평균을 사용하는 것이 적절합니다.

103
00:06:32,370 --> 00:06:34,650
중앙값은 특히 유용합니다

104
00:06:34,650 --> 00:06:37,530
이상치가 존재할 때, 왜냐하면 그것이

105
00:06:37,530 --> 00:06:40,570
극단적인 값에 더 강건하기 때문입니다,

106
00:06:40,570 --> 00:06:42,450
우리가 방금 본 것과 같은.

107
00:06:42,450 --> 00:06:44,610
마지막으로, 최빈값은 일반적으로 사용됩니다

108
00:06:44,610 --> 00:06:48,690
범주형 데이터에서, 가장 자주 나타나는

109
00:06:48,690 --> 00:06:51,450
범주를 나타냅니다.

110
00:06:51,450 --> 00:06:55,370
그럼 분산과 표준편차에 대해 이야기해보겠습니다.

111
00:06:55,370 --> 00:06:59,850
분산과 표준편차는 연속 데이터

112
00:06:59,850 --> 00:07:03,690
세트에서 값들이 어떻게 퍼져 있는지를 설명하는 두 가지 가장 일반적인 방법입니다.

113
00:07:03,690 --> 00:07:05,250
표본 분산은 표본 평균으로부터의 제곱 편차의 합을

114
00:07:05,250 --> 00:07:09,340
n에서 1을 뺀 값으로 나눈 것입니다,

115
00:07:09,340 --> 00:07:13,620
여기서 n은 표본 크기입니다.

116
00:07:13,620 --> 00:07:15,900
표준편차는 이 분산의 제곱근입니다.

117
00:07:15,900 --> 00:07:20,540
이 두 값은 개별 값들이 얼마나 평균에서 벗어나 있는지를 정량화합니다.

118
00:07:20,540 --> 00:07:24,460
표준편차가 중요한 이유는 무엇일까요?

119
00:07:24,460 --> 00:07:27,340
표준편차는 데이터 포인트가

120
00:07:27,340 --> 00:07:30,300
평균 주위에서 얼마나 퍼져 있는지를 보여줍니다.

121
00:07:30,300 --> 00:07:34,060
데이터의 일관성을 이해하는 데 도움이 됩니다.

122
00:07:34,060 --> 00:07:36,260
더 작은 값은 데이터가 밀집해 있음을 의미하고,

123
00:07:36,260 --> 00:07:40,620
더 큰 값은 더 큰 퍼짐과 더 많은

124
00:07:40,620 --> 00:07:44,340
변동을 나타냅니다.

125
00:07:44,340 --> 00:07:48,300
이는 데이터 포인트가 평균으로부터의 평균 거리를 보여줍니다.

126
00:07:48,300 --> 00:07:50,220
정규 분포에서는 특별히

127
00:07:50,220 --> 00:07:56,300
흥미로운 일이 발생합니다.

128
00:07:56,300 --> 00:07:59,380
약 68%의 데이터가 평균의 한 표준편차

129
00:07:59,380 --> 00:08:01,540
내에 있습니다--

130
00:08:01,540 --> 00:08:07,380
즉, 평균에서 한 표준편차를 뺀 것과

131
00:08:07,380 --> 00:08:09,200
더한 것 사이에 있습니다.

132
00:08:09,200 --> 00:08:13,840
게다가, 이를 평균에서 두

133
00:08:13,840 --> 00:08:18,240
표준편차를 뺀 것과 더한 것으로 확장하면,

134
00:08:18,240 --> 00:08:23,200
95%의 표본을 포함하게 됩니다.

135
00:08:23,200 --> 00:08:28,360
마지막으로, 평균에서 세

136
00:08:28,360 --> 00:08:33,360
표준편차를 뺀 것과 더한 범위를 보면,

137
00:08:33,360 --> 00:08:38,200
그 범위는 거의 모든

138
00:08:38,200 --> 00:08:43,400
표본을 포함하며, 약 99.7%입니다.

139
00:08:43,400 --> 00:08:45,640
이는 경험적 법칙, 즉 68-95-99.7 법칙으로 알려져 있으며,

140
00:08:45,640 --> 00:08:51,320
정규 분포에만 적용됩니다.

141
00:08:51,320 --> 00:08:59,640
양분위수와 백분위수에 대해서도 이야기해보겠습니다.

142
00:08:59,640 --> 00:09:05,200
양분위수는 데이터 세트를

143
00:09:05,200 --> 00:09:09,650
동일 크기의 구간으로 나누는 값입니다.

144
00:09:09,650 --> 00:09:13,690
예를 들어, 사분위수는 데이터를 네 개의 동등한 부분으로 나누어

145
00:09:13,690 --> 00:09:17,090
각 부분이 대략 같은 수의

146
00:09:17,090 --> 00:09:22,930
데이터 포인트를 포함하게 합니다.

147
00:09:22,930 --> 00:09:27,490
반면에, 백분위수는 데이터 세트 내에서의

148
00:09:27,490 --> 00:09:29,530
값의 위치를 나타냅니다.

149
00:09:29,530 --> 00:09:34,130
예를 들어, 우리가 100명의 학생이 있고,

150
00:09:34,130 --> 00:09:37,410
그들을 사분위수로 나누면,

151
00:09:37,410 --> 00:09:41,210
각 사분위수에는 25명의 학생이 있게 됩니다.

152
00:09:41,210 --> 00:09:44,410
그 다음 우리는 이러한 데이터 포인트를 백분위수로 참조할 수 있으며,

153
00:09:44,410 --> 00:09:49,130
이는 데이터 세트 내에서

154
00:09:49,130 --> 00:09:54,810
값의 순위 위치를 나타냅니다.

155
00:09:54,810 --> 00:09:57,090
상자 그림은 중요한 시각화 도구입니다

156
00:09:57,090 --> 00:10:01,010
왜냐하면 다섯 가지 주요 통계를 요약하기 때문입니다.

157
00:10:01,010 --> 00:10:04,450
여기서 첫 번째 사분위수를 볼 수 있습니다,

158
00:10:04,450 --> 00:10:08,990
이는 25번째 백분위를 나타내며,

159
00:10:08,990 --> 00:10:12,510
즉, 데이터 값의 25%가 이 지점 아래에 있습니다.

160
00:10:12,510 --> 00:10:16,550
중앙값은 두 번째 사분위수로,

161
00:10:16,550 --> 00:10:23,790
즉, 50번째 백분위수로, 데이터의 절반이

162
00:10:23,790 --> 00:10:27,230
그 값을 아래에 위치하고 절반이 위에 위치하고 있음을 나타냅니다.

163
00:10:27,230 --> 00:10:32,230
세 번째 사분위수는 75번째 백분위수로,

164
00:10:32,230 --> 00:10:37,470
즉, 데이터의 75%가 이 값 아래에 있고 25%

165
00:10:37,470 --> 00:10:42,350
이상입니다.

166
00:10:42,350 --> 00:10:48,470
그리고 네 번째와 다섯 번째 주요 통계로,

167
00:10:48,470 --> 00:10:51,190
우리는 또한 이상치를 포함하지 않는 최소값과 최대값을 볼 수 있습니다.

168
00:10:51,190 --> 00:10:54,790
이로 인해 데이터의 중심, 분포,

169
00:10:54,790 --> 00:10:59,350
그리고 잠재적 이상치를 점으로 시각화하는 데 도움이 됩니다.

