1
00:00:05,053 --> 00:00:06,220
강사: 여러분, 안녕하세요.

2
00:00:06,220 --> 00:00:07,520
다시 만나서 반갑습니다.

3
00:00:07,520 --> 00:00:09,760
오늘 강의에서는

4
00:00:09,760 --> 00:00:14,120
데이터 탐색과 분석을 위한 통계에 대해 이야기하겠습니다.

5
00:00:14,120 --> 00:00:16,880
간단한 질문으로 시작해 봅시다.

6
00:00:16,880 --> 00:00:18,920
통계란 무엇일까요?

7
00:00:18,920 --> 00:00:22,680
형식적으로, 통계는 다음에 초점을 맞춘 수학의 한 분야입니다

8
00:00:22,680 --> 00:00:25,540
데이터를 수집, 분석, 해석하고

9
00:00:25,540 --> 00:00:29,920
통찰을 도출하고 더 나은 의사결정을 지원하도록

10
00:00:29,920 --> 00:00:31,720
데이터를 제시하는 것에.

11
00:00:31,720 --> 00:00:36,360
이번 강의에서는 기술통계에 집중하겠습니다. 기술통계는

12
00:00:36,360 --> 00:00:39,120
데이터를 요약하고 시각화하여

13
00:00:39,120 --> 00:00:42,480
패턴과 추세를 드러내는 데 도움이 되는 방법들입니다.

14
00:00:42,480 --> 00:00:45,040
예시로 시작해 봅시다.

15
00:00:45,040 --> 00:00:48,400
Universal AI에 재학 중인 학생들의 키를

16
00:00:48,400 --> 00:00:51,560
연구하고 싶다고 가정해 봅시다.

17
00:00:51,560 --> 00:00:56,720
UAI의 모든 학생의 키를 일일이 측정하는 대신,

18
00:00:56,720 --> 00:00:58,400
표본을 추출합니다--

19
00:00:58,400 --> 00:01:02,650
예를 들어, 무작위로 선정한 100명의 학생.

20
00:01:02,650 --> 00:01:07,730
이렇게 하면 키 값 100개로 구성된 데이터 세트를 얻게 됩니다.

21
00:01:07,730 --> 00:01:11,090
이제 이 데이터를 시각화해 볼 수 있습니다,

22
00:01:11,090 --> 00:01:15,370
예를 들어, 표본에 포함된 모든 학생의 키를

23
00:01:15,370 --> 00:01:17,690
나란히 그려보는 방식으로요.

24
00:01:17,690 --> 00:01:20,770
물론, 값을 정렬해서

25
00:01:20,770 --> 00:01:26,050
추세를 파악할 수 있는데, 대부분의 학생들의 키가 비슷하다는 점입니다.

26
00:01:26,050 --> 00:01:29,810
그렇다면 전형적인 학생의 키는 얼마일까요?

27
00:01:29,810 --> 00:01:33,650
이를 설명하는 일반적인 방법은 세 가지가 있습니다.

28
00:01:33,650 --> 00:01:38,350
산술 평균 키, 즉 평균을 계산할 수 있고,

29
00:01:38,350 --> 00:01:41,290
그 값이 65인치입니다.

30
00:01:41,290 --> 00:01:43,770
다음으로, 중앙값을 찾을 수 있는데,

31
00:01:43,770 --> 00:01:47,890
이는 데이터를 정렬했을 때 가운데 값을 의미합니다.

32
00:01:47,890 --> 00:01:52,530
즉, 50명의 학생은 이 값보다 작고 50명의 학생은

33
00:01:52,530 --> 00:01:55,530
이 값보다 큽니다.

34
00:01:55,530 --> 00:01:58,890
마지막으로 최빈값도 식별할 수 있는데,

35
00:01:58,890 --> 00:02:02,410
가장 자주 나타나는 값입니다.

36
00:02:02,410 --> 00:02:07,070
이 키 값들을 동일한 크기의 구간으로 묶고

37
00:02:07,070 --> 00:02:10,389
각 구간에 몇 명의 학생이 속하는지 셉니다.

38
00:02:10,389 --> 00:02:12,750
이러한 계수와 구간화하는 과정은

39
00:02:12,750 --> 00:02:15,870
분포를 탐색하는 데 사용되는 시각화 기법의

40
00:02:15,870 --> 00:02:18,990
기반이 됩니다.

41
00:02:18,990 --> 00:02:22,790
여기서 대부분의 학생 키가 어디에 집중되어 있는지

42
00:02:22,790 --> 00:02:26,670
그리고 범위 전반에 어떻게 퍼져 있는지 볼 수 있습니다.

43
00:02:26,670 --> 00:02:29,210
몇몇 개인은 비교적 키가 크지만,

44
00:02:29,210 --> 00:02:33,390
대부분의 키는 중심 주변에 군집합니다.

45
00:02:33,390 --> 00:02:36,370
이제 개별 점 대신,

46
00:02:36,370 --> 00:02:41,670
막대그래프를 사용해 히스토그램을 만들 수 있습니다.

47
00:02:41,670 --> 00:02:46,390
히스토그램은 각 구간에서 값이 얼마나 자주 발생하는지,

48
00:02:46,390 --> 00:02:49,910
즉, 우리 사례에서는 키 범위별 빈도를 보여줍니다.

49
00:02:49,910 --> 00:02:54,910
이는 표본의 분포에 대한 명확한 그림을 제공하며,

50
00:02:54,910 --> 00:03:00,190
대다수 학생들이 어디에 몰려 있고 나머지는 어떻게 퍼져 있는지 보여줍니다.

51
00:03:00,190 --> 00:03:03,230
구간 크기--

52
00:03:03,230 --> 00:03:06,040
간격 크기--를 변경하여

53
00:03:06,040 --> 00:03:09,120
데이터에서 얼마나 많은 변동성이 보이는지 확인할 수 있습니다.

54
00:03:09,120 --> 00:03:11,600
더 작은 구간 크기는

55
00:03:11,600 --> 00:03:15,880
더 높은 세분성에서의 변화를 보여주는 반면, 더 큰 구간 크기는

56
00:03:15,880 --> 00:03:18,200
가시적인 세부 정보를 줄입니다.

57
00:03:18,200 --> 00:03:22,120
그래서 구간이 너무 작으면 결과가

58
00:03:22,120 --> 00:03:26,020
노이즈가 많아질 수 있고, 너무 크면

59
00:03:26,020 --> 00:03:30,080
분포가 지나치게 단순화될 수 있습니다.

60
00:03:30,080 --> 00:03:34,920
또한 커널 밀도 추정,

61
00:03:34,920 --> 00:03:39,080
즉 KDE를 사용해 데이터를 제시할 수도 있는데, 이는 전통적인 히스토그램의

62
00:03:39,080 --> 00:03:41,760
부드러운 대안을 제공합니다.

63
00:03:41,760 --> 00:03:46,920
히스토그램과 달리, KDE는 구간 크기에 의존하지 않으며

64
00:03:46,920 --> 00:03:50,720
데이터 분포의 형태를

65
00:03:50,720 --> 00:03:53,240
더 명확하게 시각화해 줍니다.

66
00:03:53,240 --> 00:03:59,440
또 다른 대안은 바이올린 플롯으로, 이는 KDE를 좌우 대칭으로 반영한 것입니다.

67
00:03:59,440 --> 00:04:03,120
이는 두 분포를 비교하는 데 유용하며,

68
00:04:03,120 --> 00:04:06,260
우리 예시에서 남학생과 여학생의 키 분포처럼

69
00:04:06,260 --> 00:04:09,220
비교할 수 있습니다.

70
00:04:09,220 --> 00:04:13,780
데이터의 형태에 관해 말하자면, 왜도는

71
00:04:13,780 --> 00:04:19,899
데이터 분포의 비대칭성, 즉 기울어짐을 포착하는 유용한 측도입니다.

72
00:04:19,899 --> 00:04:23,940
이는 데이터가 편향되어 있는지, 또는 분포의 한쪽에 치우쳐 있는지를

73
00:04:23,940 --> 00:04:25,980
이해하는 데 도움을 줍니다.

74
00:04:25,980 --> 00:04:29,020
예를 들어, 점들이 중심을 기준으로 고르게

75
00:04:29,020 --> 00:04:31,380
분포되어 있다면, 데이터는

76
00:04:31,380 --> 00:04:34,300
대칭적이며 왜도가 없습니다.

77
00:04:34,300 --> 00:04:37,740
대부분의 데이터 포인트가 왼쪽에 군집하고,

78
00:04:37,740 --> 00:04:43,020
오른쪽으로 긴 꼬리가 있다면, 이를 양의 왜도라고 합니다.

79
00:04:43,020 --> 00:04:45,980
대부분의 포인트가 오른쪽에 군집하고,

80
00:04:45,980 --> 00:04:50,260
왼쪽으로 긴 꼬리가 있다면, 이는 음의 왜도입니다.

81
00:04:50,260 --> 00:04:52,980
완벽하게 대칭적인 분포에서는,

82
00:04:52,980 --> 00:04:57,300
평균, 중앙값, 최빈값이 동일합니다.

83
00:04:57,300 --> 00:05:01,300
하지만 비대칭적이거나 왜도 있는 분포에서는,

84
00:05:01,300 --> 00:05:04,500
이 세 가지 측도가 서로 다릅니다.

85
00:05:04,500 --> 00:05:07,590
이제 이상치에 대해 이야기해 봅시다.

86
00:05:07,590 --> 00:05:13,710
이상치는 데이터 세트에서 비정상적으로 크거나 작은 값입니다.

87
00:05:13,710 --> 00:05:17,050
많은 경우에 이상치는 예상 가능하지만,

88
00:05:17,050 --> 00:05:19,670
오류로 인해 발생할 수도 있습니다.

89
00:05:19,670 --> 00:05:22,830
예를 들어, 우리가 학생들의 키에 대한 연구를

90
00:05:22,830 --> 00:05:26,190
학생들에게 자신의 키를 보고하도록 요청하는 방식으로 수행한다고 해봅시다. 그런데 한 학생이

91
00:05:26,190 --> 00:05:29,390
장난으로 자신의 키를 에펠탑의 높이로

92
00:05:29,390 --> 00:05:34,630
보고했다고 합시다.

93
00:05:34,630 --> 00:05:37,230
이제 결과를 시각화하면, 히스토그램이

94
00:05:37,230 --> 00:05:39,590
이와 비슷하게 보일 수 있습니다.

95
00:05:39,590 --> 00:05:41,650
우리의 평균이 이제 왜곡되었습니다.

96
00:05:41,650 --> 00:05:45,470
190인치를 넘는 수준으로 올라갑니다.

97
00:05:45,470 --> 00:05:51,550
이는 단 하나의 이상치가 평균에 얼마나 큰 영향을 미칠 수 있는지 보여줍니다.

98
00:05:51,550 --> 00:05:56,230
하지만 중앙값은 여전히 약 65인치에 머물며,

99
00:05:56,230 --> 00:06:00,630
데이터의 진정한 중심 경향을 반영합니다.

100
00:06:00,630 --> 00:06:05,030
데이터셋을 살펴보면, 이 극도로 높은

101
00:06:05,030 --> 00:06:08,290
값이 명백히 오류라는 것을 알 수 있습니다.

102
00:06:08,290 --> 00:06:12,730
효과적으로 데이터를 분석하기 위해 이것을 안전하게 제거할 수 있습니다.

103
00:06:12,730 --> 00:06:16,330
그래서 항상 시각적으로 점검하는 것이 중요한 이유입니다

104
00:06:16,330 --> 00:06:21,250
분석을 수행하거나 결론을 내리기 전에 데이터를.

105
00:06:21,250 --> 00:06:24,290
모든 것이 말이 되는지 확인하세요.

106
00:06:24,290 --> 00:06:27,850
그럼 기술통계로 돌아가서,

107
00:06:27,850 --> 00:06:32,370
평균은 데이터가 대칭적일 때 사용하는 것이 적절합니다.

108
00:06:32,370 --> 00:06:34,650
중앙값은 특히 유용한데,

109
00:06:34,650 --> 00:06:37,530
왜냐하면 이상치가 존재할 때

110
00:06:37,530 --> 00:06:40,570
극단값에 더 강건하기 때문이며,

111
00:06:40,570 --> 00:06:42,450
방금 본 것 같은 값에 대해서도 마찬가지입니다.

112
00:06:42,450 --> 00:06:44,610
마지막으로 최빈값은 일반적으로

113
00:06:44,610 --> 00:06:48,690
범주형 데이터에서 사용되며, 가장 자주

114
00:06:48,690 --> 00:06:51,450
나타나는 범주를 나타냅니다.

115
00:06:51,450 --> 00:06:55,370
그럼 분산과 표준편차에 대해 이야기해 봅시다.

116
00:06:55,370 --> 00:06:59,850
분산과 표준편차는 연속형 데이터에서 값들이 얼마나

117
00:06:59,850 --> 00:07:03,690
퍼져 있는지를 설명하는 가장 일반적인 두 가지 방법입니다.

118
00:07:03,690 --> 00:07:05,250
세트에서.

119
00:07:05,250 --> 00:07:09,340
표본분산은 표본평균으로부터의 편차들을 제곱해 합한 값에

120
00:07:09,340 --> 00:07:13,620
n 마이너스 1로 나눈 값이며,

121
00:07:13,620 --> 00:07:15,900
여기서 n은 표본 크기입니다.

122
00:07:15,900 --> 00:07:20,540
표준편차는 이 분산의 제곱근입니다.

123
00:07:20,540 --> 00:07:24,460
이 둘 모두 개별 값들이

124
00:07:24,460 --> 00:07:27,340
평균으로부터 얼마나 벗어나는지를 정량화합니다.

125
00:07:27,340 --> 00:07:30,300
표준편차는 왜 중요할까요?

126
00:07:30,300 --> 00:07:34,060
표준편차는 데이터 점들이 평균을 중심으로

127
00:07:34,060 --> 00:07:36,260
얼마나 퍼져 있는지를 보여줍니다.

128
00:07:36,260 --> 00:07:40,620
이는 데이터의 일관성을 이해하는 데 도움이 됩니다.

129
00:07:40,620 --> 00:07:44,340
값이 작을수록 데이터가 촘촘히 모여 있고,

130
00:07:44,340 --> 00:07:48,300
값이 클수록 더 큰 분산과 더 많은

131
00:07:48,300 --> 00:07:50,220
변동을 나타냅니다.

132
00:07:50,220 --> 00:07:56,300
이는 데이터 점이 평균으로부터 떨어진 평균 거리를 보여줍니다.

133
00:07:56,300 --> 00:07:59,380
정규분포에서는 특히

134
00:07:59,380 --> 00:08:01,540
흥미로운 일이 일어납니다.

135
00:08:01,540 --> 00:08:07,380
약 68%의 데이터가 평균으로부터 한 표준편차

136
00:08:07,380 --> 00:08:09,200
이내에 위치합니다—

137
00:08:09,200 --> 00:08:13,840
즉, 평균 마이너스 1 표준편차와

138
00:08:13,840 --> 00:08:18,240
평균 플러스 1 표준편차 사이입니다.

139
00:08:18,240 --> 00:08:23,200
더 나아가, 이를 평균 마이너스 두

140
00:08:23,200 --> 00:08:28,360
표준편차와 평균 플러스 두 표준편차로 확장하면,

141
00:08:28,360 --> 00:08:33,360
표본의 95%를 포함하게 됩니다.

142
00:08:33,360 --> 00:08:38,200
마지막으로, 평균 마이너스 세

143
00:08:38,200 --> 00:08:43,400
표준편차에서 평균 플러스 세 표준편차까지의 범위를 보면,

144
00:08:43,400 --> 00:08:45,640
그 범위는 거의 모든

145
00:08:45,640 --> 00:08:51,320
표본, 즉 대략 99.7%를 포함합니다.

146
00:08:51,320 --> 00:08:59,640
이는 경험법칙, 혹은 68-95-99.7 법칙으로 알려져 있으며,

147
00:08:59,640 --> 00:09:05,200
정규분포에만 적용됩니다.

148
00:09:05,200 --> 00:09:09,650
또한 분위수와 백분위수에 대해서도 이야기해 봅시다.

149
00:09:09,650 --> 00:09:13,690
분위수는 데이터셋을

150
00:09:13,690 --> 00:09:17,090
동일한 크기의 구간으로 나누는 값입니다.

151
00:09:17,090 --> 00:09:22,930
예를 들어, 사분위수는 데이터를 네 개의 동등한 부분으로 나누어

152
00:09:22,930 --> 00:09:27,490
각 부분이 대략 동일한 수의

153
00:09:27,490 --> 00:09:29,530
데이터 점을 포함하도록 합니다.

154
00:09:29,530 --> 00:09:34,130
반면, 백분위수는 데이터셋 내에서의 값의 위치를

155
00:09:34,130 --> 00:09:37,410
나타냅니다.

156
00:09:37,410 --> 00:09:41,210
예를 들어 학생이 100명이 있다면,

157
00:09:41,210 --> 00:09:44,410
그들을 사분위수로 나누었을 때,

158
00:09:44,410 --> 00:09:49,130
각 사분위수에는 25명의 학생이 있게 됩니다.

159
00:09:49,130 --> 00:09:54,810
그 다음 우리는 이러한 데이터 점들을 백분위수를 사용해 지칭할 수 있는데,

160
00:09:54,810 --> 00:09:57,090
이는 데이터셋 내에서의 값의 서열 위치를

161
00:09:57,090 --> 00:10:01,010
나타냅니다.

162
00:10:01,010 --> 00:10:04,450
상자그림은 다섯 가지 핵심 통계를 요약해 주기 때문에

163
00:10:04,450 --> 00:10:08,990
중요한 시각화 도구입니다.

164
00:10:08,990 --> 00:10:12,510
여기에서 첫 번째 사분위수를 볼 수 있는데,

165
00:10:12,510 --> 00:10:16,550
이는 25번째 백분위수에 해당하며,

166
00:10:16,550 --> 00:10:23,790
데이터 값의 25%가 이 지점 아래에 있다는 뜻입니다.

167
00:10:23,790 --> 00:10:27,230
두 번째 사분위수인 중앙값은,

168
00:10:27,230 --> 00:10:32,230
혹은 50번째 백분위수로, 데이터의 절반이

169
00:10:32,230 --> 00:10:37,470
그 값보다 낮고 절반은 높다는 것을 나타냅니다.

170
00:10:37,470 --> 00:10:42,350
세 번째 사분위수는 75번째 백분위수로,

171
00:10:42,350 --> 00:10:48,470
데이터의 75%가 이 값보다 낮고 25%는

172
00:10:48,470 --> 00:10:51,190
높다는 뜻입니다.

173
00:10:51,190 --> 00:10:54,790
그리고 네 번째와 다섯 번째 핵심 통계로서,

174
00:10:54,790 --> 00:10:59,350
플롯에서 이상치를 포함하지 않는 최솟값과 최댓값도

175
00:10:59,350 --> 00:11:03,190
볼 수 있습니다.

176
00:11:03,190 --> 00:11:07,330
따라서 이것은 또한 중심, 분산,

177
00:11:07,330 --> 00:11:11,500
그리고 잠재적 이상치를 데이터에서 점으로 시각화하는 데 도움을 줍니다.

